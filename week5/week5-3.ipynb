{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "628623a7",
   "metadata": {},
   "source": [
    "# DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa8892ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfad4eb8",
   "metadata": {},
   "source": [
    "## DataFrame은 행,열로 구조화된 데이터구조이다. \n",
    "\n",
    "### RDD에 schema를 얹은 구조\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e170bbd",
   "metadata": {},
   "source": [
    "- NullType\n",
    "- StringType\n",
    "- BinaryType\n",
    "- BooleanType\n",
    "- DateType\n",
    "- TimestampType\n",
    "- DoubleType\n",
    "- DecimalType\n",
    "- ShortType\n",
    "- ArrayType\n",
    "- MapType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51d03c7",
   "metadata": {},
   "source": [
    "# DataFrame 생성\n",
    "\n",
    "## schema를 정해주어야 하나, 정해주지 않으면 알아서 정해줌\n",
    "### schema는 row와 column으로 구성됨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4e7c3c",
   "metadata": {},
   "source": [
    "#### 외부에서 읽기 : spark.createDataFrame() 또는 spark.read()\n",
    "#### 내부에서 읽기 : spark.createDataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eecb0ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "myList=[('1','kim, js', 170),\n",
    "        ('1','lee, sm', 175),\n",
    "        ('2','lim, yg',180),\n",
    "        ('2','lee', 170)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6947e2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(myList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7df98129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_1', '_2', '_3']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "503faf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      " |-- _3: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87986480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(_1='1', _2='kim, js', _3=170)]\n"
     ]
    }
   ],
   "source": [
    "print (myDf.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df7d14a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['year','name','height']\n",
    "_myDf = spark.createDataFrame(myList, cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74f9c102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['year', 'name', 'height']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_myDf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10b26ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(year='1', name='kim, js', height=170)]\n"
     ]
    }
   ],
   "source": [
    "print (_myDf.take(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73f9939c",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"kim\",\"lee\",\"lee\",\"lim\"]\n",
    "items = [\"espresso\",\"latte\",\"americano\",\"affocato\",\"long black\",\"macciato\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c269e275",
   "metadata": {},
   "outputs": [],
   "source": [
    "coffeeDf = spark.createDataFrame([(names[i%4], items[i%6]) for i in range(100)],\\\n",
    "                           [\"name\",\"coffee\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2084bca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- coffee: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coffeeDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7aca63f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|name|    coffee|\n",
      "+----+----------+\n",
      "| kim|  espresso|\n",
      "| lee|     latte|\n",
      "| lee| americano|\n",
      "| lim|  affocato|\n",
      "| kim|long black|\n",
      "| lee|  macciato|\n",
      "| lee|  espresso|\n",
      "| lim|     latte|\n",
      "| kim| americano|\n",
      "| lee|  affocato|\n",
      "+----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coffeeDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cd756eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "Person = Row('year','name', 'height')\n",
    "row1=Person('1','kim, js',170)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9a5d538f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row1:  1 kim, js 170\n"
     ]
    }
   ],
   "source": [
    "print (\"row1: \", row1.year, row1.name, row1.height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7f91328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'year': '1', 'name': 'kim, js', 'height': 170}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row1.asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e438248c",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRows = [row1,\n",
    "          Person('1','lee, sm', 175),\n",
    "          Person('2','lim, yg',180),\n",
    "          Person('2','lee',170)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1ceec51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(myRows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a56e98bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: long (nullable = true)\n",
      "\n",
      "None\n",
      "+----+-------+------+\n",
      "|year|   name|height|\n",
      "+----+-------+------+\n",
      "|   1|kim, js|   170|\n",
      "|   1|lee, sm|   175|\n",
      "|   2|lim, yg|   180|\n",
      "|   2|    lee|   170|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (myDf.printSchema())\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ec80dc",
   "metadata": {},
   "source": [
    "# 스키마 만들어서 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0baa38f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "mySchema=StructType([\n",
    "    StructField(\"year\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"height\", IntegerType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61c028e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(myRows, mySchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c81c2dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- year: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- height: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ecd6a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(year='1', name='kim, js', height=170)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myDf.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d8609756",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wcDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-b65d250279fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwcDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ClubCountry'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Position'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'wcDf' is not defined"
     ]
    }
   ],
   "source": [
    "wcDf.groupBy('ClubCountry').pivot('Position').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d5af92f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wcDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-bd2eaec1ce74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwcDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupBy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwcDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mClubCountry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'wcDf' is not defined"
     ]
    }
   ],
   "source": [
    "wcDf.groupBy(wcDf.ClubCountry).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b60794d",
   "metadata": {},
   "source": [
    "# F함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "875f8bef",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'heightD'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-81a41fcb199c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmyDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheightD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheightD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheightD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheightD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1641\u001b[0m         \"\"\"\n\u001b[0;32m   1642\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1643\u001b[1;33m             raise AttributeError(\n\u001b[0m\u001b[0;32m   1644\u001b[0m                 \"'%s' object has no attribute '%s'\" % (self.__class__.__name__, name))\n\u001b[0;32m   1645\u001b[0m         \u001b[0mjc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'heightD'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "myDf.agg(F.min(myDf.heightD),F.max(myDf.heightD),F.avg(myDf.heightD),F.sum(myDf.heightD)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c942b9",
   "metadata": {},
   "source": [
    "# 행추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c27030e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "toAppendDf = spark.createDataFrame([Row(1, \"choi, js\", 177)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f76d88b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_myDf = myDf.union(toAppendDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1c906123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+------+\n",
      "|year|    name|height|\n",
      "+----+--------+------+\n",
      "|   1| kim, js|   170|\n",
      "|   1| lee, sm|   175|\n",
      "|   2| lim, yg|   180|\n",
      "|   2|     lee|   170|\n",
      "|   1|choi, js|   177|\n",
      "+----+--------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b394e43",
   "metadata": {},
   "source": [
    "# 결측값 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9eb4b3d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[year: string, name: string, height: int]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "myDf.where(F.col(\"height\").isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8595245a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+\n",
      "|year|name|height|\n",
      "+----+----+------+\n",
      "|   0|   0|     0|\n",
      "+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "myDf.select([count(when(isnan(c), c)).alias(c) for c in myDf.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b7608a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+\n",
      "|year|name|height|\n",
      "+----+----+------+\n",
      "|   0|   0|     0|\n",
      "+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "myDf.select([count(when(col(c).isNull(), c)).alias(c) for c in myDf.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50658d2",
   "metadata": {},
   "source": [
    "# 결측값을 채우는 함수이다.\n",
    "\n",
    "#### df.na.fill(0) 모든 컬럼의 na를 0으로 교체\n",
    "#### df.fillna( { 'c0':0, 'c1':0 } ) 컬럼 c0, c1의 na를 0으로 교체\n",
    "#### 결측값을 삭제할 수도 있다.\n",
    "\n",
    "#### df.na.drop(subset=[\"c0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919ebf76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
