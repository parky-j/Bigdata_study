{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fd87e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21be9877",
   "metadata": {},
   "source": [
    "# DataFrame 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c887428",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [[1, [1.0, 2.0, 3.0]], [1, [1.1, 2.1, 3.1]], [0, [1.2, 2.2, 3.3]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e589673b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: 1\n",
      "features: [1.0, 2.0, 3.0]\n"
     ]
    }
   ],
   "source": [
    "print (\"label: {}\\nfeatures: {}\".format(p[0][0], p[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67e917fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf=spark.createDataFrame(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77a41ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_1=1, _2=[1.0, 2.0, 3.0]),\n",
       " Row(_1=1, _2=[1.1, 2.1, 3.1]),\n",
       " Row(_1=0, _2=[1.2, 2.2, 3.3])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "083d65d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "p = [LabeledPoint(1, [1.0,2.0,3.0]),\n",
    "     LabeledPoint(1, [1.1,2.1,3.1]),\n",
    "     LabeledPoint(0, [1.2,2.2,3.3])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53079a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDf=spark.createDataFrame(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "154cc768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(features=DenseVector([1.0, 2.0, 3.0]), label=1.0),\n",
       " Row(features=DenseVector([1.1, 2.1, 3.1]), label=1.0),\n",
       " Row(features=DenseVector([1.2, 2.2, 3.3]), label=0.0)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58539bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "\n",
    "trainDf = spark.createDataFrame([\n",
    "    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n",
    "    (0.0, Vectors.dense([2.0, 1.0, 1.0])),\n",
    "    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n",
    "    (1.0, Vectors.dense([0.0, 1.2, 0.5]))], [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ceb73b77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(label=1.0, features=DenseVector([0.0, 1.1, 0.1])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.0, 1.0])),\n",
       " Row(label=0.0, features=DenseVector([2.0, 1.3, 1.0])),\n",
       " Row(label=1.0, features=DenseVector([0.0, 1.2, 0.5]))]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDf.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62cc89b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.linalg import SparseVector # ml ok\n",
    "\n",
    "_rdd = spark.sparkContext.parallelize([\n",
    "    (0.0, SparseVector(4, {1: 1.0, 3: 5.5})),\n",
    "    (1.0, SparseVector(4, {0: -1.0, 2: 0.5}))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cdc2486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df=_rdd.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a342c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "_df=_df.withColumnRenamed('_1', 'label').withColumnRenamed('_2', 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cab5c11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0| (4,[1,3],[1.0,5.5])|\n",
      "|  1.0|(4,[0,2],[-1.0,0.5])|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13068e0d",
   "metadata": {},
   "source": [
    "# 단어 빈도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "990c33fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=[\n",
    "    \"When I find myself in times of trouble\",\n",
    "    \"Mother Mary comes to me\",\n",
    "    \"Speaking words of wisdom, let it be\",\n",
    "    \"And in my hour of darkness\",\n",
    "    \"She is standing right in front of me\",\n",
    "    \"Speaking words of wisdom, let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Let it be\",\n",
    "    \"Whisper words of wisdom, let it be\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09c42d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d={}\n",
    "for sentence in doc:\n",
    "    words=sentence.split()\n",
    "    for word in words:\n",
    "        if word in d:\n",
    "            d[word]+=1\n",
    "        else:\n",
    "            d[word]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e056dad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When\t1\n",
      "I\t1\n",
      "find\t1\n",
      "myself\t1\n",
      "in\t3\n",
      "times\t1\n",
      "of\t6\n",
      "trouble\t1\n",
      "Mother\t1\n",
      "Mary\t1\n",
      "comes\t1\n",
      "to\t1\n",
      "me\t2\n",
      "Speaking\t2\n",
      "words\t3\n",
      "wisdom,\t3\n",
      "let\t3\n",
      "it\t7\n",
      "be\t7\n",
      "And\t1\n",
      "my\t1\n",
      "hour\t1\n",
      "darkness\t1\n",
      "She\t1\n",
      "is\t1\n",
      "standing\t1\n",
      "right\t1\n",
      "front\t1\n",
      "Let\t4\n",
      "Whisper\t1\n"
     ]
    }
   ],
   "source": [
    "for k,v in d.items():\n",
    "    print (\"{}\\t{}\".format(k,v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28cb3f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2d=[\n",
    "    [\"When I find myself in times of trouble\"],\n",
    "    [\"Mother Mary comes to me\"],\n",
    "    [\"Speaking words of wisdom, let it be\"],\n",
    "    [\"And in my hour of darkness\"],\n",
    "    [\"She is standing right in front of me\"],\n",
    "    [\"Speaking words of wisdom, let it be\"],\n",
    "    [u\"우리 Let it be\"],\n",
    "    [u\"나 Let it be\"],\n",
    "    [u\"너 Let it be\"],\n",
    "    [\"Let it be\"],\n",
    "    [\"Whisper words of wisdom, let it be\"]\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d83f558f",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf=spark.createDataFrame(doc2d, ['sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca13b098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                sent|\n",
      "+--------------------+\n",
      "|When I find mysel...|\n",
      "|Mother Mary comes...|\n",
      "|Speaking words of...|\n",
      "|And in my hour of...|\n",
      "|She is standing r...|\n",
      "|Speaking words of...|\n",
      "|      우리 Let it be|\n",
      "|        나 Let it be|\n",
      "|        너 Let it be|\n",
      "|           Let it be|\n",
      "|Whisper words of ...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "67d560ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"sent\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9d91fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokDf = tokenizer.transform(myDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8c87512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                sent|               words|\n",
      "+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|\n",
      "|Mother Mary comes...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokDf.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d9b2a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "\n",
    "re = RegexTokenizer(inputCol=\"sent\", outputCol=\"wordsReg\", pattern=\"\\\\s+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b5037d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|                sent|            wordsReg|\n",
      "+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|\n",
      "|Mother Mary comes...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "|And in my hour of...|[and, in, my, hou...|\n",
      "|She is standing r...|[she, is, standin...|\n",
      "|Speaking words of...|[speaking, words,...|\n",
      "|      우리 Let it be| [우리, let, it, be]|\n",
      "|        나 Let it be|   [나, let, it, be]|\n",
      "|        너 Let it be|   [너, let, it, be]|\n",
      "|           Let it be|       [let, it, be]|\n",
      "|Whisper words of ...|[whisper, words, ...|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reDf=re.transform(myDf)\n",
    "reDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc2d2e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stop = StopWordsRemover(inputCol=\"wordsReg\", outputCol=\"nostops\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d9f2ceac",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=list()\n",
    "_stopwords=stop.getStopWords()\n",
    "for e in _stopwords:\n",
    "    stopwords.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df8d6f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StopWordsRemover_118ec901a7d8"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_mystopwords=[u\"나\",u\"너\", u\"우리\"]\n",
    "for e in _mystopwords:\n",
    "    stopwords.append(e)\n",
    "stop.setStopWords(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce22ff89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i/me/my/myself/we/our/ours/ourselves/you/your/yours/yourself/yourselves/he/him/his/himself/she/her/hers/herself/it/its/itself/they/them/their/theirs/themselves/what/which/who/whom/this/that/these/those/am/is/are/was/were/be/been/being/have/has/had/having/do/does/did/doing/a/an/the/and/but/if/or/because/as/until/while/of/at/by/for/with/about/against/between/into/through/during/before/after/above/below/to/from/up/down/in/out/on/off/over/under/again/further/then/once/here/there/when/where/why/how/all/any/both/each/few/more/most/other/some/such/no/nor/not/only/own/same/so/than/too/very/s/t/can/will/just/don/should/now/i'll/you'll/he'll/she'll/we'll/they'll/i'd/you'd/he'd/she'd/we'd/they'd/i'm/you're/he's/she's/it's/we're/they're/i've/we've/you've/they've/isn't/aren't/wasn't/weren't/haven't/hasn't/hadn't/don't/doesn't/didn't/won't/wouldn't/shan't/shouldn't/mustn't/can't/couldn't/cannot/could/here's/how's/let's/ought/that's/there's/what's/when's/where's/who's/why's/would/나/너/우리/"
     ]
    }
   ],
   "source": [
    "for e in stop.getStopWords():\n",
    "    print (e, end=\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "03195164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|            wordsReg|             nostops|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[when, i, find, m...|[find, times, tro...|\n",
      "|Mother Mary comes...|[mother, mary, co...|[mother, mary, co...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|\n",
      "|And in my hour of...|[and, in, my, hou...|    [hour, darkness]|\n",
      "|She is standing r...|[she, is, standin...|[standing, right,...|\n",
      "|Speaking words of...|[speaking, words,...|[speaking, words,...|\n",
      "|      우리 Let it be| [우리, let, it, be]|               [let]|\n",
      "|        나 Let it be|   [나, let, it, be]|               [let]|\n",
      "|        너 Let it be|   [너, let, it, be]|               [let]|\n",
      "|           Let it be|       [let, it, be]|               [let]|\n",
      "|Whisper words of ...|[whisper, words, ...|[whisper, words, ...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopDf=stop.transform(reDf)\n",
    "stopDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4a7a5a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "doc = reduce(lambda x,y: x+y, doc2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01ccd914",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48403bec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 0)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 13)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 3)\t1\n",
      "  (3, 2)\t1\n",
      "  (3, 1)\t1\n",
      "  (4, 8)\t1\n",
      "  (4, 6)\t1\n",
      "  (5, 7)\t1\n",
      "  (5, 13)\t1\n",
      "  (5, 12)\t1\n",
      "  (5, 3)\t1\n",
      "  (6, 3)\t1\n",
      "  (6, 14)\t1\n",
      "  (7, 3)\t1\n",
      "  (8, 3)\t1\n",
      "  (9, 3)\t1\n",
      "  (10, 13)\t1\n",
      "  (10, 12)\t1\n",
      "  (10, 3)\t1\n",
      "  (10, 11)\t1\n"
     ]
    }
   ],
   "source": [
    "print (vectorizer.fit_transform(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7d50e7b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'times': 9,\n",
       " 'trouble': 10,\n",
       " 'mother': 5,\n",
       " 'mary': 4,\n",
       " 'comes': 0,\n",
       " 'speaking': 7,\n",
       " 'words': 13,\n",
       " 'wisdom': 12,\n",
       " 'let': 3,\n",
       " 'hour': 2,\n",
       " 'darkness': 1,\n",
       " 'standing': 8,\n",
       " 'right': 6,\n",
       " '우리': 14,\n",
       " 'whisper': 11}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "872e3295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit_transform(doc).todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b10bcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "cv = CountVectorizer(inputCol=\"nostops\", outputCol=\"cv\", vocabSize=30, minDF=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a59707c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cv.fit(stopDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7ae345c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.ml.feature.CountVectorizer'> <class 'pyspark.ml.feature.CountVectorizerModel'>\n"
     ]
    }
   ],
   "source": [
    "print (type(cv),type(cvModel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ff495b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "cvDf = cvModel.transform(stopDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "82516cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                sent|             nostops|                  cv|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|When I find mysel...|[find, times, tro...|(16,[5,6,8],[1.0,...|\n",
      "|Mother Mary comes...|[mother, mary, co...|(16,[10,13,14],[1...|\n",
      "|Speaking words of...|[speaking, words,...|(16,[0,1,2,3],[1....|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cvDf.drop('wordsReg').show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4e54434f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idf: 2.09861228866811\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "tf=1./4\n",
    "df=3.\n",
    "N=11.\n",
    "idf=math.log((N+1)/(df+1))+1\n",
    "print (\"idf: {}\".format(idf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f5fc5313",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=1.0, stop_words='english',norm = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ec96afca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10)\t2.791759469228055\n",
      "  (0, 9)\t2.791759469228055\n",
      "  (1, 0)\t2.791759469228055\n",
      "  (1, 4)\t2.791759469228055\n",
      "  (1, 5)\t2.791759469228055\n",
      "  (2, 3)\t1.4054651081081644\n",
      "  (2, 12)\t2.09861228866811\n",
      "  (2, 13)\t2.09861228866811\n",
      "  (2, 7)\t2.386294361119891\n",
      "  (3, 1)\t2.791759469228055\n",
      "  (3, 2)\t2.791759469228055\n",
      "  (4, 6)\t2.791759469228055\n",
      "  (4, 8)\t2.791759469228055\n",
      "  (5, 3)\t1.4054651081081644\n",
      "  (5, 12)\t2.09861228866811\n",
      "  (5, 13)\t2.09861228866811\n",
      "  (5, 7)\t2.386294361119891\n",
      "  (6, 14)\t2.791759469228055\n",
      "  (6, 3)\t1.4054651081081644\n",
      "  (7, 3)\t1.4054651081081644\n",
      "  (8, 3)\t1.4054651081081644\n",
      "  (9, 3)\t1.4054651081081644\n",
      "  (10, 11)\t2.791759469228055\n",
      "  (10, 3)\t1.4054651081081644\n",
      "  (10, 12)\t2.09861228866811\n",
      "  (10, 13)\t2.09861228866811\n"
     ]
    }
   ],
   "source": [
    "print (vectorizer.fit_transform(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "75ecb129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'times': 9,\n",
       " 'trouble': 10,\n",
       " 'mother': 5,\n",
       " 'mary': 4,\n",
       " 'comes': 0,\n",
       " 'speaking': 7,\n",
       " 'words': 13,\n",
       " 'wisdom': 12,\n",
       " 'let': 3,\n",
       " 'hour': 2,\n",
       " 'darkness': 1,\n",
       " 'standing': 8,\n",
       " 'right': 6,\n",
       " '우리': 14,\n",
       " 'whisper': 11}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef89549a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.79175947, 2.79175947, 2.79175947, 1.40546511, 2.79175947,\n",
       "       2.79175947, 2.79175947, 2.38629436, 2.79175947, 2.79175947,\n",
       "       2.79175947, 2.79175947, 2.09861229, 2.09861229, 2.79175947])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3dcd620d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hashTF = HashingTF(inputCol=\"nostops\", outputCol=\"hash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e595d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashDf = hashTF.transform(stopDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "14cc09b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------+\n",
      "|hash                                                    |\n",
      "+--------------------------------------------------------+\n",
      "|(262144,[64317,91878,152481],[1.0,1.0,1.0])             |\n",
      "|(262144,[24657,63767,245426],[1.0,1.0,1.0])             |\n",
      "|(262144,[27556,151864,173339,175131],[1.0,1.0,1.0,1.0]) |\n",
      "|(262144,[74517,98431],[1.0,1.0])                        |\n",
      "|(262144,[84798,218360,229166],[1.0,1.0,1.0])            |\n",
      "|(262144,[27556,151864,173339,175131],[1.0,1.0,1.0,1.0]) |\n",
      "|(262144,[173339],[1.0])                                 |\n",
      "|(262144,[173339],[1.0])                                 |\n",
      "|(262144,[173339],[1.0])                                 |\n",
      "|(262144,[173339],[1.0])                                 |\n",
      "|(262144,[151864,173339,175131,188139],[1.0,1.0,1.0,1.0])|\n",
      "+--------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashDf.select(\"hash\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "14dc2e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = hashDf.select(\"hash\").rdd.map(lambda x:x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b6dad09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "\n",
    "word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"words\", outputCol=\"w2v\")\n",
    "model = word2Vec.fit(tokDf)\n",
    "w2vDf = model.transform(tokDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c372ccd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------------------------------------------------+\n",
      "|word    |vector                                                            |\n",
      "+--------+------------------------------------------------------------------+\n",
      "|trouble |[0.08801643550395966,0.03672950714826584,-0.1287679374217987]     |\n",
      "|mother  |[0.11748811602592468,-0.02989991381764412,-0.11452135443687439]   |\n",
      "|find    |[-0.13126887381076813,-0.027504412457346916,0.07296513020992279]  |\n",
      "|standing|[-0.06447822600603104,-0.020746180787682533,0.11937719583511353]  |\n",
      "|wisdom, |[-0.1087152287364006,0.045862145721912384,-0.1298976093530655]    |\n",
      "|in      |[0.11704662442207336,0.07607141882181168,-0.08325448632240295]    |\n",
      "|myself  |[-0.05106130614876747,-0.07791944593191147,-0.15875688195228577]  |\n",
      "|is      |[-0.04754877835512161,-0.02113082818686962,-0.0039343019016087055]|\n",
      "|darkness|[-0.1577366590499878,0.0678299218416214,-0.09224570542573929]     |\n",
      "|우리    |[-0.030192559584975243,-0.07797732949256897,-0.11803052574396133] |\n",
      "|front   |[0.10727176070213318,-0.1300991028547287,0.0038653474766761065]   |\n",
      "|it      |[-0.14590394496917725,0.13722999393939972,-0.056479379534721375]  |\n",
      "|너      |[-0.058282557874917984,0.04401526600122452,0.11258867383003235]   |\n",
      "|she     |[-0.052308451384305954,0.10389576852321625,-0.06569955497980118]  |\n",
      "|comes   |[0.05429510027170181,-0.1212816908955574,-0.05899808555841446]    |\n",
      "|i       |[-0.019710591062903404,-0.1619323492050171,0.06562304496765137]   |\n",
      "|hour    |[-0.08156338334083557,0.09551865607500076,0.031825654208660126]   |\n",
      "|to      |[0.1304951012134552,-0.05913205444812775,0.16546495258808136]     |\n",
      "|speaking|[0.11360758543014526,-0.11027339845895767,0.07804454118013382]    |\n",
      "|mary    |[0.08141773194074631,0.12734642624855042,-0.13858725130558014]    |\n",
      "+--------+------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.getVectors().show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "073b0e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+\n",
      "|    word|        similarity|\n",
      "+--------+------------------+\n",
      "|      it|0.9208083748817444|\n",
      "|darkness|0.8202618956565857|\n",
      "| wisdom,|0.8098458051681519|\n",
      "+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.findSynonyms(\"she\", 3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "84ff71ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "ngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "feb2e5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramDf = ngram.transform(tokDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "adfce10f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+----------------------+\n",
      "|                sent|               words|                ngrams|\n",
      "+--------------------+--------------------+----------------------+\n",
      "|When I find mysel...|[when, i, find, m...|  [when i, i find, ...|\n",
      "|Mother Mary comes...|[mother, mary, co...|  [mother mary, mar...|\n",
      "|Speaking words of...|[speaking, words,...|  [speaking words, ...|\n",
      "|And in my hour of...|[and, in, my, hou...|  [and in, in my, m...|\n",
      "|She is standing r...|[she, is, standin...|  [she is, is stand...|\n",
      "|Speaking words of...|[speaking, words,...|  [speaking words, ...|\n",
      "|      우리 Let it be| [우리, let, it, be]|[우리 let, let it, ...|\n",
      "|        나 Let it be|   [나, let, it, be]| [나 let, let it, i...|\n",
      "|        너 Let it be|   [너, let, it, be]| [너 let, let it, i...|\n",
      "|           Let it be|       [let, it, be]|       [let it, it be]|\n",
      "|Whisper words of ...|[whisper, words, ...|  [whisper words, w...|\n",
      "+--------------------+--------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ngramDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "681eb48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(words=['when', 'i', 'find', 'myself', 'in', 'times', 'of', 'trouble'], ngrams=['when i', 'i find', 'find myself', 'myself in', 'in times', 'times of', 'of trouble'])\n",
      "Row(words=['mother', 'mary', 'comes', 'to', 'me'], ngrams=['mother mary', 'mary comes', 'comes to', 'to me'])\n",
      "Row(words=['speaking', 'words', 'of', 'wisdom,', 'let', 'it', 'be'], ngrams=['speaking words', 'words of', 'of wisdom,', 'wisdom, let', 'let it', 'it be'])\n"
     ]
    }
   ],
   "source": [
    "for e in ngramDf.select(\"words\",\"ngrams\").take(3):\n",
    "    print (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4f2ee7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"sent\", outputCol=\"sentLabel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "08816203",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=labelIndexer.fit(myDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e29704f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "siDf=model.transform(myDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "13dc7721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+\n",
      "|                sent|sentLabel|\n",
      "+--------------------+---------+\n",
      "|When I find mysel...|      5.0|\n",
      "|Mother Mary comes...|      3.0|\n",
      "|Speaking words of...|      0.0|\n",
      "|And in my hour of...|      1.0|\n",
      "|She is standing r...|      4.0|\n",
      "|Speaking words of...|      0.0|\n",
      "|      우리 Let it be|      9.0|\n",
      "|        나 Let it be|      7.0|\n",
      "|        너 Let it be|      8.0|\n",
      "|           Let it be|      2.0|\n",
      "|Whisper words of ...|      6.0|\n",
      "+--------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "siDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8971e98d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark_heightweight.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_heightweight.txt\n",
    "1   65.78   112.99\n",
    "2   71.52   136.49\n",
    "3   69.40   153.03\n",
    "4   68.22   142.34\n",
    "5   67.79   144.30\n",
    "6   68.70   123.30\n",
    "7   69.80   141.49\n",
    "8   70.01   136.46\n",
    "9   67.90   112.37\n",
    "10  66.78   120.67\n",
    "11  66.49   127.45\n",
    "12  67.62   114.14\n",
    "13  68.30   125.61\n",
    "14  67.12   122.46\n",
    "15  68.28   116.09\n",
    "16  71.09   140.00\n",
    "17  66.46   129.50\n",
    "18  68.65   142.97\n",
    "19  71.23   137.90\n",
    "20  67.13   124.04\n",
    "21  67.83   141.28\n",
    "22  68.88   143.54\n",
    "23  63.48   97.90\n",
    "24  68.42   129.50\n",
    "25  67.63   141.85\n",
    "26  67.21   129.72\n",
    "27  70.84   142.42\n",
    "28  67.49   131.55\n",
    "29  66.53   108.33\n",
    "30  65.44   113.89\n",
    "31  69.52   103.30\n",
    "32  65.81   120.75\n",
    "33  67.82   125.79\n",
    "34  70.60   136.22\n",
    "35  71.80   140.10\n",
    "36  69.21   128.75\n",
    "37  66.80   141.80\n",
    "38  67.66   121.23\n",
    "39  67.81   131.35\n",
    "40  64.05   106.71\n",
    "41  68.57   124.36\n",
    "42  65.18   124.86\n",
    "43  69.66   139.67\n",
    "44  67.97   137.37\n",
    "45  65.98   106.45\n",
    "46  68.67   128.76\n",
    "47  66.88   145.68\n",
    "48  67.70   116.82\n",
    "49  69.82   143.62\n",
    "50  69.09   134.93"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "012a7b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import os\n",
    "\n",
    "rdd=spark.sparkContext\\\n",
    "    .textFile(os.path.join('data','ds_spark_heightweight.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b7454489",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd=rdd.map(lambda line:[float(x) for x in line.split('\\t')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4f94b2a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 25) (192.168.55.202 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 604, in main\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 596, in process\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 1560, in takeUpToNumLeft\n    yield next(iterator)\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-68-5adff9afa2e9>\", line 1, in <lambda>\n  File \"<ipython-input-68-5adff9afa2e9>\", line 1, in <listcomp>\nValueError: could not convert string to float: '1   65.78   112.99'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 604, in main\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 596, in process\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 1560, in takeUpToNumLeft\n    yield next(iterator)\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-68-5adff9afa2e9>\", line 1, in <lambda>\n  File \"<ipython-input-68-5adff9afa2e9>\", line 1, in <listcomp>\nValueError: could not convert string to float: '1   65.78   112.99'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-69-367820fd5b9f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmyDf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyRdd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"weight\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"height\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36mcreateDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    673\u001b[0m             return super(SparkSession, self).createDataFrame(\n\u001b[0;32m    674\u001b[0m                 data, schema, samplingRatio, verifySchema)\n\u001b[1;32m--> 675\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    676\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_create_dataframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverifySchema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    697\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 698\u001b[1;33m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromRDD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    699\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m             \u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_createFromLocal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprepare\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_createFromRDD\u001b[1;34m(self, rdd, schema, samplingRatio)\u001b[0m\n\u001b[0;32m    484\u001b[0m         \"\"\"\n\u001b[0;32m    485\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mschema\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m             \u001b[0mstruct\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_inferSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamplingRatio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    487\u001b[0m             \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_create_converter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstruct\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[0mrdd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\session.py\u001b[0m in \u001b[0;36m_inferSchema\u001b[1;34m(self, rdd, samplingRatio, names)\u001b[0m\n\u001b[0;32m    458\u001b[0m         \u001b[1;33m:\u001b[0m\u001b[1;32mclass\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStructType\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m         \"\"\"\n\u001b[1;32m--> 460\u001b[1;33m         \u001b[0mfirst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfirst\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    461\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfirst\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m             raise ValueError(\"The first row in RDD is empty, \"\n",
      "\u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mfirst\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1584\u001b[0m         \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mRDD\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mempty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1585\u001b[0m         \"\"\"\n\u001b[1;32m-> 1586\u001b[1;33m         \u001b[0mrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1587\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1588\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1565\u001b[0m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartsScanned\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnumPartsToTry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotalParts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1566\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtakeUpToNumLeft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1568\u001b[0m             \u001b[0mitems\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36mrunJob\u001b[1;34m(self, rdd, partitionFunc, partitions, allowLocal)\u001b[0m\n\u001b[0;32m   1231\u001b[0m         \u001b[1;31m# SparkContext#runJob.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1232\u001b[0m         \u001b[0mmappedRDD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmapPartitions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpartitionFunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1233\u001b[1;33m         \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunJob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1234\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmappedRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1235\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 1 times, most recent failure: Lost task 0.0 in stage 26.0 (TID 25) (192.168.55.202 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 604, in main\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 596, in process\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 1560, in takeUpToNumLeft\n    yield next(iterator)\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-68-5adff9afa2e9>\", line 1, in <lambda>\n  File \"<ipython-input-68-5adff9afa2e9>\", line 1, in <listcomp>\nValueError: could not convert string to float: '1   65.78   112.99'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.api.python.PythonRDD$.runJob(PythonRDD.scala:166)\r\n\tat org.apache.spark.api.python.PythonRDD.runJob(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 604, in main\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 596, in process\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\rdd.py\", line 1560, in takeUpToNumLeft\n    yield next(iterator)\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-68-5adff9afa2e9>\", line 1, in <lambda>\n  File \"<ipython-input-68-5adff9afa2e9>\", line 1, in <listcomp>\nValueError: could not convert string to float: '1   65.78   112.99'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.api.python.PythonRDD$.$anonfun$runJob$1(PythonRDD.scala:166)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "myDf=spark.createDataFrame(myRdd,[\"id\",\"weight\",\"height\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "c034248b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = spark.createDataFrame([\n",
    "        (0, \"a b c d e spark\", 1.0),\n",
    "        (1, \"b d\", 0.0),\n",
    "        (2, \"spark f g h\", 1.0),\n",
    "        (3, \"hadoop mapreduce\", 0.0),\n",
    "        (4, \"my dog has flea problems. help please.\",0.0)\n",
    "    ], [\"id\", \"text\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d2ee5478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, Tokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9e8d3882",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "0282d62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "102cb27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "myDf = model.transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ded83d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  1.0|(262144,[74920,89...|\n",
      "|  0.0|(262144,[89530,14...|\n",
      "|  1.0|(262144,[36803,17...|\n",
      "|  0.0|(262144,[132966,1...|\n",
      "|  0.0|(262144,[1074,389...|\n",
      "+-----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myDf.select('label', 'features').show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a748e9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
