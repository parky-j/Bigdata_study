{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "598a1a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "myConf=pyspark.SparkConf()\n",
    "spark = pyspark.sql.SparkSession.builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"myApp\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dadd5a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "cereRdd = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ceremony.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d4f0f298",
   "metadata": {},
   "outputs": [],
   "source": [
    "cereRdd = cereRdd.filter(lambda x:len(x)!=0)\\\n",
    "                .map(lambda x:[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0d0ec571",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['존경하는 국민 여러분, 경찰관 여러분, 일흔네 돌 ‘경찰의 날’입니다.'],\n",
       " ['국민의 안전을 위해 밤낮없이 애쓰시는 전국의 15만 경찰관 여러분께 먼저 감사를 드립니다. 전몰·순직 경찰관들의 고귀한 희생에 경의를 표합니다. 유가족 여러분께 위로의 마음을 전합니다.'],\n",
       " ['오늘 홍조근정훈장을 받으신 중앙경찰학교장 이은정 치안감님, 근정포장을 받으신 광주남부경찰서 김동현 경감님을 비롯한 수상자 여러분께 각별한 축하와 감사를 드립니다. 또한 경찰 영웅으로 추서되신 차일혁, 최중락님께 국민의 사랑을 전해드립니다.'],\n",
       " ['사랑하는 경찰관 여러분,'],\n",
       " ['여러분의 헌신적 노력으로 우리의 치안은 참 좋아졌습니다. 지난해 범죄 발생은 2015년에 비해 15.1% 줄었습니다. 같은 기간 교통사고 사망자는 18.2% 감소했습니다.'],\n",
       " ['치안의 개선은 국민의 체감으로 나타나고 있습니다. 올해 상반기 국민의 체감안전도는 74.5점으로 역대 최고를 기록했습니다. 범죄안전도는 처음으로 80점을 넘었습니다.'],\n",
       " ['한국을 찾는 외국 관광객들도 우리의 치안을 가장 좋게 평가합니다. 한국의 무엇이 좋았느냐는 물음에 외국 관광객들은 7년 연속으로 치안이 가장 좋았다고 응답했습니다. 개발도상국들은 우리의 경찰을 모범으로 삼으려 합니다.'],\n",
       " ['올해는 ‘경찰의 날’에 맞춰 국제치안산업박람회와 서울국제경찰청장회의가 함께 열립니다. 우리의 치안 발전과 치안산업 발전이 세계에 더 널리 알려지게 될 것입니다.'],\n",
       " ['자랑스러운 경찰관 여러분,'],\n",
       " ['경찰헌장은 “나라와 겨레를 위하여 충성”을 다한다는 다짐으로 시작합니다. 헌장처럼 우리 경찰은 ‘나라와 겨레를 위한 충성’의 길을 걸으려 노력해 왔습니다.'],\n",
       " ['대한민국 경찰은 1945년 광복 직후에 공식 탄생했습니다. 그러나 그 뿌리는 대한민국 임시정부에 닿아 있습니다.'],\n",
       " ['임시정부 초대 경무국장 백범 김구 선생과 나석주, 나창헌, 유상근 의사 등 임시정부 경찰은 앞장서서 일제와 싸웠습니다. 일본 관헌에게 폭탄을 던지고, 밀정을 응징하며, 임정 요인들을 보호했습니다.'],\n",
       " ['광복 이후 6·25전쟁에서도 경찰은 국군과 함께 피를 흘렸습니다. 그 전쟁에서 1만여 명의 경찰관이 목숨을 잃었습니다. 그 후로도 경찰은 국민의 안전을 지키고 국가의 안보를 도왔습니다. 역대 경찰의 헌신에 대해 국민과 함께 거듭 감사의 말씀을 드립니다. 감사합니다.'],\n",
       " ['그러나 잘못도 없지는 않았습니다. 한때 경찰은 공권력을 무리하게 집행하며 국민의 인권을 훼손했습니다. 부실하거나 불공정한 수사로 국민의 지탄을 받은 적도 있습니다. 무기력한 법 집행으로 국민께 걱정을 드리기도 했습니다.'],\n",
       " ['지금 경찰은 과거를 돌아보며 국민과 국가에 충성하는 경찰로 거듭나려고 노력하고 있습니다. 경찰은 문재인 정부 들어 가장 먼저 개혁위원회를 만들고 자체개혁에 나섰습니다. 경찰의 개혁을 국민은 큰 기대로 주목하고 있습니다.'],\n",
       " ['검경 수사권 조정과 자치경찰제 도입이 국회에서 논의되고 있습니다. 국회가 조속히 입법을 매듭지어 주시기 바랍니다. 그리하여 경찰이 중립성, 공정성, 전문성을 갖추고 본연의 임무를 충실히 수행하는 선진경찰로 더욱 발전해 주기를 소망합니다.'],\n",
       " ['정부는 경찰의 근무여건을 개선하기 위해 노력하고 있습니다. 이미 경찰관 8,572명을 늘렸고, 앞으로도 충원을 계속할 것입니다. 특히 일선 경찰의 근무환경을 개선하겠습니다.'],\n",
       " ['정부는 누구도 법 위에 군림하지 못하는 법치주의를 확립하고자 합니다. 그러자면 검찰과 경찰이 법을 누구에게나 엄정하고 공정하게 집행해야 됩니다. 수사 또한 엄정하고 공정해야 합니다. 동시에 검찰과 경찰 스스로도 법을 엄격히 준수해야 합니다. 특히 공권력이 인권의 제약을 수반하는 경우에는 절제하며 행사하는 것이 마땅합니다. 검찰개혁과 경찰개혁은 더 미룰 수 없는 시대적 과제가 됐습니다.'],\n",
       " ['경찰헌장은 따뜻한 경찰, 의로운 경찰, 공정한 경찰을 다짐합니다. 흔들림 없이 그 길로 가시기 바랍니다. 국민이 여러분을 응원할 것입니다.'],\n",
       " ['행사를 준비하신 민갑룡 경찰청장님과 관계자 여러분, 고맙습니다. 함께하신 진영 행정안전부 장관님, 이용범 인천시의회 의장님, 박정훈 경찰위원장님, 강영규 경우회장님과 역대 경찰청장님, 그리고 우리 시민 경찰님들을 비롯한 내빈 여러분, 고맙습니다.'],\n",
       " ['감사합니다.']]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cereRdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba678f4",
   "metadata": {},
   "source": [
    "# 문제 1의 답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "555f5a05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cereDf=spark.createDataFrame(cereRdd.collect(), ['sent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e77df94f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+\n",
      "|                              sent|\n",
      "+----------------------------------+\n",
      "|  존경하는 국민 여러분, 경찰관 ...|\n",
      "| 국민의 안전을 위해 밤낮없이 애...|\n",
      "|오늘 홍조근정훈장을 받으신 중앙...|\n",
      "|           사랑하는 경찰관 여러분,|\n",
      "|여러분의 헌신적 노력으로 우리의...|\n",
      "| 치안의 개선은 국민의 체감으로 ...|\n",
      "| 한국을 찾는 외국 관광객들도 우...|\n",
      "|   올해는 ‘경찰의 날’에 맞춰 국...|\n",
      "|         자랑스러운 경찰관 여러분,|\n",
      "| 경찰헌장은 “나라와 겨레를 위하...|\n",
      "|    대한민국 경찰은 1945년 광복...|\n",
      "| 임시정부 초대 경무국장 백범 김...|\n",
      "|    광복 이후 6·25전쟁에서도 경...|\n",
      "|그러나 잘못도 없지는 않았습니다...|\n",
      "| 지금 경찰은 과거를 돌아보며 국...|\n",
      "| 검경 수사권 조정과 자치경찰제 ...|\n",
      "|정부는 경찰의 근무여건을 개선하...|\n",
      "| 정부는 누구도 법 위에 군림하지...|\n",
      "| 경찰헌장은 따뜻한 경찰, 의로운...|\n",
      "|행사를 준비하신 민갑룡 경찰청장...|\n",
      "+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cereDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bab78f",
   "metadata": {},
   "source": [
    "# 문제 2의 답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "452f658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "tokenizer = Tokenizer(inputCol=\"sent\", outputCol=\"words\")\n",
    "tokDf = tokenizer.transform(cereDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1cb20b83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+--------------------------------+\n",
      "|                              sent|                           words|\n",
      "+----------------------------------+--------------------------------+\n",
      "|  존경하는 국민 여러분, 경찰관 ...|   [존경하는, 국민, 여러분,, ...|\n",
      "| 국민의 안전을 위해 밤낮없이 애...|  [국민의, 안전을, 위해, 밤낮...|\n",
      "|오늘 홍조근정훈장을 받으신 중앙...|[오늘, 홍조근정훈장을, 받으신...|\n",
      "|           사랑하는 경찰관 여러분,|     [사랑하는, 경찰관, 여러분,]|\n",
      "|여러분의 헌신적 노력으로 우리의...| [여러분의, 헌신적, 노력으로,...|\n",
      "| 치안의 개선은 국민의 체감으로 ...|  [치안의, 개선은, 국민의, 체...|\n",
      "| 한국을 찾는 외국 관광객들도 우...|  [한국을, 찾는, 외국, 관광객...|\n",
      "|   올해는 ‘경찰의 날’에 맞춰 국...|    [올해는, ‘경찰의, 날’에, ...|\n",
      "|         자랑스러운 경찰관 여러분,|   [자랑스러운, 경찰관, 여러분,]|\n",
      "| 경찰헌장은 “나라와 겨레를 위하...| [경찰헌장은, “나라와, 겨레를...|\n",
      "|    대한민국 경찰은 1945년 광복...|    [대한민국, 경찰은, 1945년...|\n",
      "| 임시정부 초대 경무국장 백범 김...|  [임시정부, 초대, 경무국장, ...|\n",
      "|    광복 이후 6·25전쟁에서도 경...|    [광복, 이후, 6·25전쟁에서...|\n",
      "|그러나 잘못도 없지는 않았습니다...|  [그러나, 잘못도, 없지는, 않...|\n",
      "| 지금 경찰은 과거를 돌아보며 국...|  [지금, 경찰은, 과거를, 돌아...|\n",
      "| 검경 수사권 조정과 자치경찰제 ...|  [검경, 수사권, 조정과, 자치...|\n",
      "|정부는 경찰의 근무여건을 개선하...| [정부는, 경찰의, 근무여건을,...|\n",
      "| 정부는 누구도 법 위에 군림하지...|   [정부는, 누구도, 법, 위에,...|\n",
      "| 경찰헌장은 따뜻한 경찰, 의로운...|  [경찰헌장은, 따뜻한, 경찰,,...|\n",
      "|행사를 준비하신 민갑룡 경찰청장...|  [행사를, 준비하신, 민갑룡, ...|\n",
      "+----------------------------------+--------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd838710",
   "metadata": {},
   "source": [
    "# 문제 3의 답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "aeb7d977",
   "metadata": {},
   "outputs": [],
   "source": [
    "_stopwords = ['돌', '참', '날', '더', '될', '등', '그', '법', '큰']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1dfd9495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "stop = StopWordsRemover(inputCol=\"words\", outputCol=\"nostops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2242760f",
   "metadata": {},
   "source": [
    "# 문제 4의 답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7e4797b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+--------------------------------+--------------------------------+\n",
      "|                              sent|                           words|                         nostops|\n",
      "+----------------------------------+--------------------------------+--------------------------------+\n",
      "|  존경하는 국민 여러분, 경찰관 ...|   [존경하는, 국민, 여러분,, ...|   [존경하는, 국민, 여러분,, ...|\n",
      "| 국민의 안전을 위해 밤낮없이 애...|  [국민의, 안전을, 위해, 밤낮...|  [국민의, 안전을, 위해, 밤낮...|\n",
      "|오늘 홍조근정훈장을 받으신 중앙...|[오늘, 홍조근정훈장을, 받으신...|[오늘, 홍조근정훈장을, 받으신...|\n",
      "|           사랑하는 경찰관 여러분,|     [사랑하는, 경찰관, 여러분,]|     [사랑하는, 경찰관, 여러분,]|\n",
      "|여러분의 헌신적 노력으로 우리의...| [여러분의, 헌신적, 노력으로,...| [여러분의, 헌신적, 노력으로,...|\n",
      "| 치안의 개선은 국민의 체감으로 ...|  [치안의, 개선은, 국민의, 체...|  [치안의, 개선은, 국민의, 체...|\n",
      "| 한국을 찾는 외국 관광객들도 우...|  [한국을, 찾는, 외국, 관광객...|  [한국을, 찾는, 외국, 관광객...|\n",
      "|   올해는 ‘경찰의 날’에 맞춰 국...|    [올해는, ‘경찰의, 날’에, ...|    [올해는, ‘경찰의, 날’에, ...|\n",
      "|         자랑스러운 경찰관 여러분,|   [자랑스러운, 경찰관, 여러분,]|   [자랑스러운, 경찰관, 여러분,]|\n",
      "| 경찰헌장은 “나라와 겨레를 위하...| [경찰헌장은, “나라와, 겨레를...| [경찰헌장은, “나라와, 겨레를...|\n",
      "|    대한민국 경찰은 1945년 광복...|    [대한민국, 경찰은, 1945년...|    [대한민국, 경찰은, 1945년...|\n",
      "| 임시정부 초대 경무국장 백범 김...|  [임시정부, 초대, 경무국장, ...|  [임시정부, 초대, 경무국장, ...|\n",
      "|    광복 이후 6·25전쟁에서도 경...|    [광복, 이후, 6·25전쟁에서...|    [광복, 이후, 6·25전쟁에서...|\n",
      "|그러나 잘못도 없지는 않았습니다...|  [그러나, 잘못도, 없지는, 않...|  [그러나, 잘못도, 없지는, 않...|\n",
      "| 지금 경찰은 과거를 돌아보며 국...|  [지금, 경찰은, 과거를, 돌아...|  [지금, 경찰은, 과거를, 돌아...|\n",
      "| 검경 수사권 조정과 자치경찰제 ...|  [검경, 수사권, 조정과, 자치...|  [검경, 수사권, 조정과, 자치...|\n",
      "|정부는 경찰의 근무여건을 개선하...| [정부는, 경찰의, 근무여건을,...| [정부는, 경찰의, 근무여건을,...|\n",
      "| 정부는 누구도 법 위에 군림하지...|   [정부는, 누구도, 법, 위에,...|   [정부는, 누구도, 법, 위에,...|\n",
      "| 경찰헌장은 따뜻한 경찰, 의로운...|  [경찰헌장은, 따뜻한, 경찰,,...|  [경찰헌장은, 따뜻한, 경찰,,...|\n",
      "|행사를 준비하신 민갑룡 경찰청장...|  [행사를, 준비하신, 민갑룡, ...|  [행사를, 준비하신, 민갑룡, ...|\n",
      "+----------------------------------+--------------------------------+--------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stopDf=stop.transform(tokDf)\n",
    "stopDf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "340b24ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hashTF = HashingTF(inputCol=\"nostops\", outputCol=\"hash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6e1763b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashDf = hashTF.transform(stopDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2416aa9",
   "metadata": {},
   "source": [
    "# 문제 5의 답"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "e7fb7873",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = IDF(inputCol=\"hash\", outputCol=\"idf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4c7811ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "idfModel = idf.fit(hashDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e5b7b969",
   "metadata": {},
   "outputs": [],
   "source": [
    "idfDf = idfModel.transform(hashDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0945baf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(hash=SparseVector(262144, {162: 1.0, 80732: 1.0, 118171: 2.0, 127225: 1.0, 142775: 1.0, 160086: 1.0, 172380: 1.0, 254275: 1.0}))\n",
      "Row(hash=SparseVector(262144, {5341: 1.0, 6304: 1.0, 30732: 1.0, 39431: 1.0, 43098: 1.0, 49855: 1.0, 51468: 1.0, 63600: 1.0, 75300: 1.0, 77757: 1.0, 89318: 1.0, 96799: 1.0, 110980: 1.0, 123553: 1.0, 160081: 1.0, 160086: 1.0, 167255: 1.0, 178931: 1.0, 208192: 1.0, 217323: 2.0, 257249: 1.0, 261393: 1.0}))\n",
      "Row(hash=SparseVector(262144, {29823: 1.0, 32228: 1.0, 36822: 1.0, 39431: 1.0, 41144: 1.0, 49855: 1.0, 61014: 1.0, 61103: 1.0, 64713: 1.0, 72971: 1.0, 82902: 1.0, 84159: 1.0, 89227: 1.0, 118725: 2.0, 153204: 1.0, 167255: 1.0, 178229: 1.0, 185989: 1.0, 206065: 1.0, 217323: 1.0, 235673: 1.0, 237148: 1.0, 248593: 1.0, 249689: 1.0, 251574: 1.0, 254458: 1.0}))\n",
      "Row(hash=SparseVector(262144, {118171: 1.0, 160086: 1.0, 220702: 1.0}))\n",
      "Row(hash=SparseVector(262144, {11202: 1.0, 28414: 1.0, 51063: 1.0, 53633: 1.0, 63981: 1.0, 80937: 1.0, 84279: 1.0, 92349: 1.0, 93327: 1.0, 108997: 1.0, 110381: 1.0, 112697: 1.0, 155248: 1.0, 170244: 1.0, 197092: 1.0, 199006: 1.0, 200419: 1.0, 211370: 1.0, 251648: 1.0, 256489: 1.0}))\n",
      "Row(hash=SparseVector(262144, {883: 1.0, 9986: 1.0, 23318: 1.0, 41303: 1.0, 53553: 1.0, 61945: 1.0, 102540: 1.0, 106234: 1.0, 107414: 1.0, 111232: 1.0, 141269: 1.0, 161878: 1.0, 166402: 1.0, 167255: 2.0, 185470: 1.0, 206644: 1.0, 259429: 1.0}))\n",
      "Row(hash=SparseVector(262144, {7183: 1.0, 23924: 1.0, 36207: 1.0, 37813: 1.0, 39110: 2.0, 59928: 1.0, 67651: 2.0, 67934: 1.0, 69304: 1.0, 82394: 1.0, 101149: 1.0, 109857: 1.0, 112700: 1.0, 123220: 1.0, 123862: 1.0, 139075: 1.0, 152763: 1.0, 168845: 1.0, 175280: 1.0, 178320: 1.0, 194867: 1.0, 209250: 1.0, 233769: 1.0, 256489: 2.0}))\n",
      "Row(hash=SparseVector(262144, {554: 1.0, 4988: 1.0, 37710: 1.0, 42545: 1.0, 63264: 1.0, 88322: 1.0, 93085: 1.0, 100399: 1.0, 111751: 1.0, 116270: 1.0, 118366: 1.0, 127677: 1.0, 142775: 1.0, 144706: 1.0, 144939: 1.0, 168718: 1.0, 215575: 1.0, 217049: 1.0, 256489: 1.0}))\n",
      "Row(hash=SparseVector(262144, {118171: 1.0, 160086: 1.0, 198165: 1.0}))\n",
      "Row(hash=SparseVector(262144, {4628: 1.0, 20124: 1.0, 20610: 1.0, 21623: 1.0, 60208: 1.0, 80957: 1.0, 93250: 1.0, 96552: 1.0, 105470: 1.0, 140832: 1.0, 153488: 1.0, 154116: 2.0, 163447: 1.0, 170991: 1.0, 173425: 1.0, 207217: 1.0, 223474: 1.0, 254593: 1.0}))\n"
     ]
    }
   ],
   "source": [
    "for e in idfDf.select(\"hash\").take(10):\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "4a4cb1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_rdd = idfDf.select(\"hash\").rdd.map(lambda x:x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "39a827d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(262144, {162: 1.0, 80732: 1.0, 118171: 2.0, 127225: 1.0, 142775: 1.0, 160086: 1.0, 172380: 1.0, 254275: 1.0})"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_rdd.collect()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9ad78b78",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 45.0 failed 1 times, most recent failure: Lost task 0.0 in stage 45.0 (TID 45) (192.168.55.202 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 604, in main\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 596, in process\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-116-bf4a3394933e>\", line 3, in <lambda>\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\ml\\linalg\\__init__.py\", line 724, in __getitem__\n    raise TypeError(\nTypeError: Indices must be of type integer, got type <class 'slice'>\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor55.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 604, in main\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 596, in process\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-116-bf4a3394933e>\", line 3, in <lambda>\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\ml\\linalg\\__init__.py\", line 724, in __getitem__\n    raise TypeError(\nTypeError: Indices must be of type integer, got type <class 'slice'>\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-c215ea8ae564>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_trainRdd0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    947\u001b[0m         \"\"\"\n\u001b[0;32m    948\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 949\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    950\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 45.0 failed 1 times, most recent failure: Lost task 0.0 in stage 45.0 (TID 45) (192.168.55.202 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 604, in main\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 596, in process\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-116-bf4a3394933e>\", line 3, in <lambda>\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\ml\\linalg\\__init__.py\", line 724, in __getitem__\n    raise TypeError(\nTypeError: Indices must be of type integer, got type <class 'slice'>\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2258)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2207)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2206)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2206)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1079)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1079)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2445)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2387)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2376)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2196)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2217)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2236)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2261)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat jdk.internal.reflect.GeneratedMethodAccessor55.invoke(Unknown Source)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 604, in main\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 596, in process\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 259, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-116-bf4a3394933e>\", line 3, in <lambda>\n  File \"C:\\Spark\\spark-3.1.2-bin-hadoop3.2\\python\\lib\\pyspark.zip\\pyspark\\ml\\linalg\\__init__.py\", line 724, in __getitem__\n    raise TypeError(\nTypeError: Indices must be of type integer, got type <class 'slice'>\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:517)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:652)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:635)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:470)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:941)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:941)\r\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\r\n\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\r\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\r\n\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:315)\r\n\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:313)\r\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:307)\r\n\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:307)\r\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\r\n\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:294)\r\n\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:288)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1030)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "type(_trainRdd0.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18c9029",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
