{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c99bbddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\user\\anaconda3\\lib\\site-packages (3.1.2)\n",
      "Requirement already satisfied: py4j==0.10.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dc10374d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: py4j in c:\\users\\user\\anaconda3\\lib\\site-packages (0.10.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install py4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c747ba68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "myConf=pyspark.SparkConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "de36c783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"]=\"C:\\\\Users\\\\user\\\\anaconda3\\\\python.exe\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"]=\"C:\\\\Users\\\\user\\\\anaconda3\\\\python.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01a6a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local\")\\\n",
    "    .appName(\"mmm\")\\\n",
    "    .config(conf=myConf)\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dbed7cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version \t: 3.1.2\n",
      "Spark App \t: mmm\n",
      "Spark Master \t: local\n",
      "Spark Host \t: 192.168.55.202\n"
     ]
    }
   ],
   "source": [
    "print (\"Spark version \\t: {}\".format(spark.version))\n",
    "print (\"Spark App \\t: {}\".format(spark.conf.get('spark.app.name')))\n",
    "print (\"Spark Master \\t: {}\".format(spark.conf.get('spark.master')))\n",
    "print (\"Spark Host \\t: {}\".format(spark.conf.get('spark.driver.host')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94f95bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x1bb166e9580>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext._conf.set(\"spark.app.name\", 'myApp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b492323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version \t: 3.1.2\n",
      "Spark App \t: mmm\n",
      "Spark Master \t: local\n",
      "Spark Host \t: 192.168.55.202\n"
     ]
    }
   ],
   "source": [
    "print (\"Spark version \\t: {}\".format(spark.version))\n",
    "print (\"Spark App \\t: {}\".format(spark.conf.get('spark.app.name')))\n",
    "print (\"Spark Master \\t: {}\".format(spark.conf.get('spark.master')))\n",
    "print (\"Spark Host \\t: {}\".format(spark.conf.get('spark.driver.host')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1c95a6bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spark.master', 'local'),\n",
       " ('spark.driver.host', '192.168.55.202'),\n",
       " ('spark.app.name', 'myApp'),\n",
       " ('spark.app.startTime', '1631820090157'),\n",
       " ('spark.app.id', 'local-1631820092107'),\n",
       " ('spark.rdd.compress', 'True'),\n",
       " ('spark.serializer.objectStreamReset', '100'),\n",
       " ('spark.driver.port', '1102'),\n",
       " ('spark.submit.pyFiles', ''),\n",
       " ('spark.executor.id', 'driver'),\n",
       " ('spark.submit.deployMode', 'client'),\n",
       " ('spark.ui.showConsoleProgress', 'true'),\n",
       " ('spark.sql.warehouse.dir',\n",
       "  'file:/C:/Users/user/Desktop/3-2/Bigdata/201710773/spark-warehouse')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext._conf.getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "aa249632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[1] at readRDDFromFile at PythonRDD.scala:274"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myList=[1,2,3,4,5,6,7]\n",
    "myRdd1 = spark.sparkContext.parallelize(myList)\n",
    "myRdd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52ce4344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting data/ds_spark_wiki.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile data/ds_spark_wiki.txt\n",
    "Wikipedia\n",
    "Apache Spark is an open source cluster computing framework.\n",
    "아파치 스파크는 오픈 소스 클러스터 컴퓨팅 프레임워크이다.\n",
    "Apache Spark Apache Spark Apache Spark Apache Spark\n",
    "아파치 스파크 아파치 스파크 아파치 스파크 아파치 스파크\n",
    "Originally developed at the University of California, Berkeley's AMPLab,\n",
    "the Spark codebase was later donated to the Apache Software Foundation,\n",
    "which has maintained it since.\n",
    "Spark provides an interface for programming entire clusters with\n",
    "implicit data parallelism and fault-tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed9a899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "myRdd2=spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_wiki.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "67bc1326",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data/ds_spark_2cols.csv\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data/ds_spark_2cols.csv\n",
    "35, 2\n",
    "40, 27\n",
    "12, 38\n",
    "15, 31\n",
    "21, 1\n",
    "14, 19\n",
    "46, 1\n",
    "10, 34\n",
    "28, 3\n",
    "48, 1\n",
    "16, 2\n",
    "30, 3\n",
    "32, 2\n",
    "48, 1\n",
    "31, 2\n",
    "22, 1\n",
    "12, 3\n",
    "39, 29\n",
    "19, 37\n",
    "25, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "73bbe9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "myRdd4 = spark.sparkContext\\\n",
    "    .textFile(os.path.join(\"data\",\"ds_spark_2cols.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a5809ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import spark\n",
    "myList=myRdd4.take(5)\n",
    "print (type(myList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca6fd9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
